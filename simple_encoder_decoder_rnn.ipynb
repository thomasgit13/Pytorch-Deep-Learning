{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e48496d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66b3d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d34fd2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6563a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open('dataset/english_german_translation.txt','r',encoding = 'utf-8') as text_file:\n",
    "        lines = text_file.readlines()\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s).replace('.','').strip() for s in l.split('\\t')][0:2] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55cd90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the data \n",
    "MAX_LENGTH = 6\n",
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"he is\", \n",
    "    \"she is\",\n",
    "    \"you are\", \n",
    "    \"we are\", \n",
    "    \"they are\"\n",
    ")\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8e928a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 255817 sentence pairs\n",
      "Trimmed to 1311 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "ger 1090\n",
      "eng 835\n",
      "['er ist maler', 'he is a painter']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'ger', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf181dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1090, 835)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.n_words,output_lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1929eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de1b18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating all input/output sentence pairs \n",
    "training_pairs = [tensorsFromPair(i) for i in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "981c3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36872a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,dropout_p =0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.RNN(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout_p = dropout_p \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a09b2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunks(list_, b):\n",
    "    n = int(len(list_)/b)+1\n",
    "    return [list_[start::n] for start in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fb7577d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = to_chunks(training_pairs,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1cc7bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(pair_chunks[i]) for i in range(len(pair_chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "97c7bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0195e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5 \n",
    "encoder_optimizer = optim.Adam(encoder.parameters(),lr = lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),lr =lr)\n",
    "def train_epoch(training_data,encoder,decoder,criterion,lr,encoder_optimizer,decoder_optimizer):\n",
    "    for batch in tqdm(range(len(training_data))):\n",
    "        loss = 0\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        batch_samples = training_data[batch] \n",
    "        for sample in batch_samples:\n",
    "            input_tensor,target_tensor = sample\n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = target_tensor.size(0)\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            if use_teacher_forcing:\n",
    "                for di in range(target_length):\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    topv, topi = decoder_output.topk(1)\n",
    "                    pred = topi.squeeze().detach()\n",
    "                    loss += criterion(decoder_output, target_tensor[di])\n",
    "                    decoder_input = target_tensor[di]  \n",
    "            else:\n",
    "                for di in range(target_length):\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    topv, topi = decoder_output.topk(1) \n",
    "                    decoder_input = topi.squeeze().detach() \n",
    "                    pred = topi.squeeze().detach()\n",
    "                    loss += criterion(decoder_output, target_tensor[di])\n",
    "                    if decoder_input.item() == EOS_token:\n",
    "                        break\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b02ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "lr = 0.01\n",
    "hidden_size = 1280\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7e8b0b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [04:52<00:00, 26.58s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    train_epoch(training_data,encoder,decoder,criterion,lr,encoder_optimizer,decoder_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c548b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7476c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=3):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "        # print(encoder_hidden)   \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "67f43b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093a162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "956b0b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ihr seid sehr mutig\n",
      "= you are very brave\n",
      "< skate crying breathing\n",
      "\n",
      "> sie ist au er fassung\n",
      "= she is upset\n",
      "< logged careless yourself\n",
      "\n",
      "> ich bin am dosen\n",
      "= i am dozing\n",
      "< love about opponent\n",
      "\n",
      "> er ist immer frohlich\n",
      "= he is always cheerful\n",
      "< team about irresistible\n",
      "\n",
      "> er isst gerade zu mittag\n",
      "= he is having lunch\n",
      "< logged colleague butterfly\n",
      "\n",
      "> wir lernen arabisch\n",
      "= we are learning arabic\n",
      "< butterfly daredevil family\n",
      "\n",
      "> er ist nicht mehr allein\n",
      "= he isn t alone anymore\n",
      "< logged driven japan\n",
      "\n",
      "> sie ist auf diat\n",
      "= she is on a diet\n",
      "< butterfly daredevil family\n",
      "\n",
      "> sie isst gerade\n",
      "= she is eating\n",
      "< team about irresistible\n",
      "\n",
      "> er hat ihr gegenuber vorurteile\n",
      "= he is prejudiced against her\n",
      "< butterfly daredevil dieting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e6810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8cbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca9c90c9b299e3c35d28bc96236d8f2c0bd3d51256cb5ad616950692d4a1a879"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
